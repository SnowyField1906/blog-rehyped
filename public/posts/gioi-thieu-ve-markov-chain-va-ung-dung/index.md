---
title: Gi·ªõi thi·ªáu v·ªÅ Markov Chain v√† ·ª©ng d·ª•ng
date: '2023-11-08'
tags: ['Natural Language Processing', 'Machine Learning', 'Probability']
description: Gi·ªõi thi·ªáu t·ªïng quan v·ªÅ Markov Chain v√† c√°c ·ª©ng d·ª•ng, h∆∞·ªõng d·∫´n chi ti·∫øt c√°ch t√≠nh Markov Chain v√† tri·ªÉn khai b·∫±ng Python
---

_Markov Chain l√† m·ªôt m√¥ h√¨nh x√°c su·∫•t ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ m√¥ t·∫£ c√°c qu√° tr√¨nh ng·∫´u nhi√™n d·ª±a tr√™n tr·∫°ng th√°i tr∆∞·ªõc ƒë√≥ Tuy m√¥ h√¨nh n√†y ƒë∆°n gi·∫£n nh∆∞ng n√≥ l·∫°i ƒë∆∞·ª£c ·ª©ng d·ª•ng r·ªông r√£i trong h·∫ßu h·∫øt c√°c lƒ©nh v·ª±c trong ƒë·ªùi s·ªëng n√≥i chung v√† Machine Leaning n√≥i ri√™ng._

_B√†i vi·∫øt n√†y s·∫Ω gi·ªõi thi·ªáu t·ªïng quan v·ªÅ Markov Chain v√† c√°c ·ª©ng d·ª•ng c·ªßa n√≥, h∆∞·ªõng d·∫´n chi ti·∫øt c√°ch t√≠nh Markov Chain v√† tri·ªÉn khai b·∫±ng Python._

## Kh√°i ni·ªám

**Markov Chain** (Chu·ªói Markov) l√† m·ªôt m√¥ h√¨nh x√°c su·∫•t m√¥ t·∫£ m·ªôt chu·ªói c√°c s·ª± ki·ªán c√≥ th·ªÉ x·∫£y ra. ƒê·∫∑c ƒëi·ªÉm quan tr·ªçng c·ªßa Markov Chain l√† t√≠nh **memorylessness** (kh√¥ng c√≥ tr√≠ nh·ªõ), nghƒ©a l√† x√°c su·∫•t c·ªßa m·ªôt s·ª± ki·ªán ti·∫øp theo ch·ªâ ph·ª• thu·ªôc v√†o s·ª± ki·ªán hi·ªán t·∫°i, c√°c s·ª± ki·ªán tr∆∞·ªõc ƒë√≥ s·∫Ω kh√¥ng ƒë∆∞·ª£c ghi nh·ªõ.

Hay n√≥i c√°ch kh√°c, s·ª± ph√¢n b·ªë c·ªßa c√°c tr·∫°ng th√°i t∆∞∆°ng lai ch·ªâ ph·ª• thu·ªôc v√†o tr·∫°ng th√°i hi·ªán t·∫°i ch·ª© kh√¥ng ph·ª• thu·ªôc v√†o c√°ch n√≥ ƒë·∫øn tr·∫°ng th√°i hi·ªán t·∫°i, t√≠nh ch·∫•t n√†y c√≤n ƒë∆∞·ª£c g·ªçi l√† **Markov Property** hay **Markovian** (T√≠nh Markov).

M·ªôt v√°n c·ªù c√≥ t√≠nh Markov, v√¨ x√°c su·∫•t chi·∫øn th·∫Øng ch·ªâ ph·ª• thu·ªôc v√†o v·ªã tr√≠ hi·ªán t·∫°i c·ªßa c√°c qu√¢n c·ªù, kh√¥ng ph·ª• thu·ªôc v√†o c√°c n∆∞·ªõc ƒëi tr∆∞·ªõc ƒë√≥. Trong khi ƒë√≥, h√†nh ƒë·ªông l·∫•y b√≥ng trong m·ªôt chi·∫øc h·ªôp c√≥ th·ªÉ kh√¥ng c√≥ t√≠nh Markov, v√¨ x√°c su·∫•t l·∫•y ƒë∆∞·ª£c qu·∫£ b√≥ng c·∫ßn t√¨m ph·ª• thu·ªôc v√†o c√°c qu·∫£ b√≥ng ƒë√£ l·∫•y tr∆∞·ªõc ƒë√≥.

## C√°c th√†nh ph·∫ßn c·ªßa Markov Chain

Gi·∫£ s·ª≠ c√≥ m·ªôt d·ªãch v·ª• thu√™ xe ƒë·∫°p g·ªìm 3 tr·∫°m: $A$, $B$ v√† $C$.

T·∫•t c·∫£ xe ƒë·∫°p ph·∫£i ƒë∆∞·ª£c tr·∫£ l·∫°i tr·∫°m v√†o cu·ªëi ng√†y t·∫°i m·ªôt tr·∫°m n√†o ƒë√≥ trong 3 tr·∫°m tr√™n. Sau khi ki·ªÉm tra t·∫•t c·∫£ c√°c tr·∫°m v√†o m·ªói cu·ªëi ng√†y. Ta nh·∫≠n th·∫•y r·∫±ng:

-   Trong c√°c xe ƒë·∫°p m∆∞·ª£n t·ª´ tr·∫°m $A$, c√≥ $30\%$ ƒë∆∞·ª£c tr·∫£ l·∫°i tr·∫°m $A$, $50\%$ ƒë·∫øn tr·∫°m $B$ v√† $20\%$ ƒë·∫øn tr·∫°m $C$.
-   Trong c√°c xe ƒë·∫°p m∆∞·ª£n t·ª´ tr·∫°m $B$, c√≥ $10\%$ ƒë·∫øn tr·∫°m $A$, $60\%$ ƒë∆∞·ª£c tr·∫£ l·∫°i tr·∫°m $B$ v√† $30\%$ ƒë·∫øn tr·∫°m $C$.
-   Trong c√°c xe ƒë·∫°p m∆∞·ª£n t·ª´ tr·∫°m $C$, c√≥ $10\%$ ƒë·∫øn tr·∫°m $A$, $10\%$ ƒë·∫øn tr·∫°m $B$ v√† $80\%$ ƒë∆∞·ª£c tr·∫£ l·∫°i tr·∫°m $C$.

Khi ƒë√≥ ta c√≥ th·ªÉ bi·ªÉu di·ªÖn Markov Chain c·ªßa d·ªãch v·ª• thu√™ xe ƒë·∫°p n√†y nh∆∞ h√¨nh b√™n d∆∞·ªõi.

<figure>
<img
    className="w-full md:w-1/2 flex justify-center mx-auto"
    src="/static/images/posts/markov-example.png"
    alt="M√¥ ph·ªèng v√≠ d·ª• v·ªÅ Markov Chain"
/>
<figcaption>Source: math.libretexts.org</figcaption>
</figure>

Qua ƒë√≥, ta c√≥ th·ªÉ th·∫•y r·∫±ng Markov Chain c√≥ th·ªÉ ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·ªüi m·ªôt **Directed Networks** (M·∫°ng C√≥ h∆∞·ªõng), trong ƒë√≥ c√°c ƒë·ªânh l√† t·∫≠p h·ª£p c√°c **State** (Tr·∫°ng th√°i) c√≥ th·ªÉ x·∫£y ra, v·ªõi tr·ªçng s·ªë l√† x√°c su·∫•t chuy·ªÉn t·ª´ m·ªôt tr·∫°ng th√°i n√†y sang tr·∫°ng th√°i kh√°c.

> **üìù Nh·∫Øc l·∫°i**
>
> Trong [Graph Theory](https://en.wikipedia.org/wiki/Graph_theory), m·ªôt **Directed Networks** (M·∫°ng C√≥ h∆∞·ªõng) l√† s·ª± k·∫øt h·ª£p gi·ªØa **Directed Graph** (ƒê·ªì th·ªã C√≥ h∆∞·ªõng) v√† **Weighted Graph** (ƒê·ªì th·ªã C√≥ tr·ªçng s·ªë).

### State

**State** (Tr·∫°ng th√°i) l√† m·ªôt t·∫≠p h·ª£p c√°c tr·∫°ng th√°i c√≥ th·ªÉ x·∫£y ra trong Markov Chain.

·ªû v√≠ d·ª• tr√™n, ta c√≥ 3 tr·∫°ng th√°i l√† $\{A, B, C\}$, v√¨ m·ªôt chi·∫øc xe ƒë·∫°p khi mu·ªën ƒë∆∞a v·ªÅ tr·∫°m th√¨ ch·ªâ x·∫£y ra 1 trong 3 tr∆∞·ªùng h·ª£p n√†y.

### State Vector

**State Vector** (Vector Tr·∫°ng th√°i) l√† m·ªôt ma tr·∫≠n c√≥ m·ªôt h√†ng m√¥ t·∫£ x√°c su·∫•t c·ªßa m·ªói tr·∫°ng th√°i trong Markov Chain.

$$
\begin{align}
\mathbf{Q} = \begin{bmatrix}
q_1 & q_2 & \cdots & q_n
\end{bmatrix}
\end{align}
$$

·ªû v√≠ d·ª• tr√™n, gi·∫£ s·ª≠ t·∫°i th·ªùi ƒëi·ªÉm quan s√°t ban ƒë·∫ßu, ta c√≥ $30%$ xe ƒë·∫°p ·ªü tr·∫°m $A$, $45%$ ·ªü tr·∫°m $B$ v√† $25%$ ·ªü tr·∫°m $C$. Khi ƒë√≥ ta c√≥ vector tr·∫°ng th√°i ban ƒë·∫ßu l√†:

$$
\begin{align*}
\mathbf{Q}_0 = \begin{bmatrix}
0.3 & 0.45 & 0.25
\end{bmatrix}
\end{align*}
$$

### Transition Matrix

**Transition Matrix** (Ma tr·∫≠n Chuy·ªÉn ti·∫øp) l√† m·ªôt ma tr·∫≠n m√¥ t·∫£ x√°c su·∫•t chuy·ªÉn t·ª´ m·ªôt tr·∫°ng th√°i n√†y sang m·ªôt tr·∫°ng th√°i kh√°c trong Markov Chain.

$$
\begin{align}
\mathbf{P} = \begin{bmatrix}
p_{11} & p_{12} & \cdots & p_{1n} \\
p_{21} & p_{22} & \cdots & p_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
p_{n1} & p_{n2} & \cdots & p_{nn}
\end{bmatrix}
\end{align}
$$

Trong ƒë√≥ $p_{ij}$ l√† x√°c su·∫•t chuy·ªÉn t·ª´ tr·∫°ng th√°i $i$ sang tr·∫°ng th√°i $j$. C√≥ th·ªÉ k√≠ hi·ªáu l√†:

$$
\begin{align}
p_{ij} = P(X_{t+1} = \mathbf{Q}_j | X_n = \mathbf{Q}_i)
\end{align}
$$

·ªû v√≠ d·ª• tr√™n, thay v√¨ m√¥ t·∫£ b·∫±ng graph (v·ª´a kh√¥ng th√¢n thi·ªán v·ªõi m√°y t√≠nh, v·ª´a ph·ª©c t·∫°p khi c√≥ nhi·ªÅu tr·∫°ng th√°i), ta c√≥ th·ªÉ m√¥ t·∫£ b·∫±ng transition matrix sau:

$$
\begin{align*}
\mathbf{P} = \begin{bmatrix}
0.3 & 0.5 & 0.2 \\
0.1 & 0.6 & 0.3 \\
0.1 & 0.1 & 0.8
\end{bmatrix}
\end{align*}
$$

L∆∞u √Ω r·∫±ng transition matrix l√† m·ªôt ma tr·∫≠n vu√¥ng, v√† c√°c d√≤ng c·ªßa ma tr·∫≠n n√†y ph·∫£i c√≥ t·ªïng b·∫±ng $1$.

## C√°c ph√©p t√≠nh tr√™n Markov Chain

### T√¨m x√°c xu·∫•t c√°c tr·∫°ng th√°i

Quay l·∫°i v√≠ d·ª• v·ª´a r·ªìi, ta ƒë√£ c√≥ transition matrix $P$ bi·ªÉu di·ªÖn x√°c su·∫•t thay ƒë·ªïi tr·∫°m c·ªßa c√°c xe ƒë·∫°p, v√† vector tr·∫°ng th√°i ban ƒë·∫ßu $\mathbf{Q}_0$ bi·ªÉu di·ªÖn x√°c su·∫•t ph√¢n b·ªë gi·ªØa c√°c tr·∫°m ban ƒë·∫ßu.

L√∫c n√†y, ta c√≥ th·ªÉ t√≠nh ƒë∆∞·ª£c x√°c su·∫•t ph√¢n b·ªë c·ªßa c√°c chi·∫øc xe ƒë·∫°p ·ªü c√°c tr·∫°m sau 1 ng√†y b·∫±ng c√°ch t√≠nh $\mathbf{Q}_1$ nh∆∞ sau:

$$
\begin{align*}
\mathbf{Q}_1 &= \mathbf{Q}_0 \mathbf{P} \\
&= \begin{bmatrix}
0.3 & 0.45 & 0.25
\end{bmatrix}
\begin{bmatrix}
0.3 & 0.5 & 0.2 \\
0.1 & 0.6 & 0.3 \\
0.1 & 0.1 & 0.8
\end{bmatrix} \\
&= \begin{bmatrix}
0.16 & 0.445 & 0.395
\end{bmatrix}
\end{align*}
$$

V·∫≠y ta ƒë√£ t√≠nh ƒë∆∞·ª£c sau ng√†y ƒë·∫ßu ti√™n, x√°c su·∫•t c√°c xe ƒë·∫°p ·ªü c√°c tr·∫°m $A$, $B$, $C$ l·∫ßn l∆∞·ª£t l√† $16\%$, $44.5\%$ v√† $39.5\%$.

B√¢y gi·ªù, ƒë·ªÉ t√≠nh ƒë∆∞·ª£c x√°c su·∫•t ph√¢n b·ªë c·ªßa c√°c xe ƒë·∫°p ·ªü c√°c tr·∫°m sau 2 ng√†y, ta s·∫Ω t√≠nh d·ª±a tr√™n state vector c·ªßa ng√†y th·ª© nh·∫•t:

$$
\begin{align*}
\mathbf{Q}_2 &= \mathbf{Q}_1 \mathbf{P} \\
&= \begin{bmatrix}
0.16 & 0.445 & 0.395
\end{bmatrix}
\begin{bmatrix}
0.3 & 0.5 & 0.2 \\
0.1 & 0.6 & 0.3 \\
0.1 & 0.1 & 0.8
\end{bmatrix} \\
&= \begin{bmatrix}
 0.132 & 0.3865 & 0.4815
\end{bmatrix}
\end{align*}
$$

V·∫≠y ta ƒë√£ t√≠nh ƒë∆∞·ª£c sau ng√†y th·ª© hai, x√°c su·∫•t c√°c xe ƒë·∫°p ·ªü c√°c tr·∫°m $A$, $B$, $C$ l·∫ßn l∆∞·ª£t l√† $13.2\%$, $38.65\%$ v√† $48.15\%$.

T·ªõi ƒë√¢y, ta c√≥ th·ªÉ r√∫t ra ƒë∆∞·ª£c c√¥ng th·ª©c t·ªïng qu√°t ƒë·ªÉ t√≠nh x√°c su·∫•t ph√¢n b·ªë c·ªßa c√°c xe ƒë·∫°p ·ªü c√°c tr·∫°m sau $n$ ng√†y:

$$
\begin{align}
\mathbf{Q}_n &= \mathbf{Q}_{n-1} \mathbf{P} \notag \\
&= \mathbf{Q}_0 \mathbf{P}^n
\end{align}
$$

Ta ƒë√£ bi·∫øt v√¨ $\mathbf{Q}_1 = \mathbf{Q}_0 \mathbf{P}$, n√™n $\mathbf{Q}_2 = \mathbf{Q}_0 \mathbf{P} \mathbf{P} = \mathbf{Q}_0 \mathbf{P}^2$,... Tuy nhi√™n √Ω nghƒ©a th·ª±c s·ª± c·ªßa $\mathbf{P}^n$ l√† g√¨?

### T√¨m x√°c su·∫•t chuy·ªÉn tr·∫°ng th√°i

ƒê·ªÉ gi·∫£i quy·∫øt c√¢u h·ªèi tr√™n, ta s·∫Ω th·ª≠ t√¨m $\mathbf{P}^2$:

$$
\begin{align*}
\mathbf{P}^2 &= \mathbf{P} \mathbf{P} \\
&= \begin{bmatrix}
0.3 & 0.5 & 0.2 \\
0.1 & 0.6 & 0.3 \\
0.1 & 0.1 & 0.8
\end{bmatrix}
\begin{bmatrix}
0.3 & 0.5 & 0.2 \\
0.1 & 0.6 & 0.3 \\
0.1 & 0.1 & 0.8
\end{bmatrix} \\
&= \begin{bmatrix}
0.16 & 0.12 & 0.12 \\
0.47 & 0.44 & 0.19 \\
0.37 & 0.44 & 0.69
\end{bmatrix}
\end{align*}
$$

T·ª´ ƒë·ªãnh nghƒ©a c·ªßa transition matrix t·∫°i $(2)$, $\mathbf{P}$ c√≤n cho ta bi·∫øt bi·∫øt x√°c su·∫•t chuy·ªÉn tr·∫°ng th√°i c·ªßa x√°c xe ƒë·∫°p sau ng√†y ƒë·∫ßu ti√™n.

V·∫≠y, $\mathbf{P}^2$ ch√≠nh l√† ma tr·∫≠n m√¥ t·∫£ x√°c su·∫•t ph√¢n ph·ªëi c√°c tr·∫°ng th√°i sau $2$ ng√†y. V√≠ d·ª•, x√°c su·∫•t m·ªôt chi·∫øc xe ƒë·∫°p chuy·ªÉn t·ª´ tr·∫°m $A$ sang tr·∫°m $B$ sau $2$ ng√†y l√† $12\%$.

Ta c√≥ c√¥ng th·ª©c t·ªïng qu√°t ƒë·ªÉ t√≠nh x√°c su·∫•t chuy·ªÉn tr·∫°ng th√°i sau $n$ ng√†y:

$$
\begin{align}
\mathbf{P}^n
\end{align}
$$

### V√≠ d·ª• kh√°c

#### Coca vs. Pepsi

Cho r·∫±ng:

-   N·∫øu lo·∫°i n∆∞·ªõc ng·ªçt g·∫ßn nh·∫•t m√† m·ªôt ng∆∞·ªùi mua l√† $\text{Coca}$, s·∫Ω c√≥ $90\%$ kh·∫£ nƒÉng ng∆∞·ªùi ƒë√≥ s·∫Ω mua $\text{Coca}$ trong l·∫ßn mua ti·∫øp theo.
-   N·∫øu lo·∫°i n∆∞·ªõc ng·ªçt g·∫ßn nh·∫•t m√† m·ªôt ng∆∞·ªùi mua l√† $\text{Pepsi}$, s·∫Ω c√≥ $80\%$ kh·∫£ nƒÉng ng∆∞·ªùi ƒë√≥ s·∫Ω mua $\text{Pepsi}$ trong l·∫ßn mua ti·∫øp theo.

Gi·∫£ s·ª≠ m·ªôt ng∆∞·ªùi mua $\text{Coca}$ trong l·∫ßn mua ƒë·∫ßu ti√™n, h√£y t√≠nh x√°c su·∫•t ng∆∞·ªùi ƒë√≥ s·∫Ω mua $\text{Coca}$ trong l·∫ßn mua th·ª© $3$.

Ta l·∫≠p transition matrix v·ªõi $[\text{Coca}, \text{Pepsi}]$ l√† c√°c tr·∫°ng th√°i:

$$
\begin{align*}
\mathbf{P} &= \begin{bmatrix}
0.9 & 0.1 \\
0.2 & 0.8
\end{bmatrix}
\end{align*}
$$

V·ªõi:

-   $P(\text{Coca} \rightarrow \text{Coca}) = 0.9$
-   $P(\text{Pepsi} \rightarrow \text{Pepsi}) = 0.8$
-   $P(\text{Pepsi} \rightarrow \text{Coca}) = 0.2$

Khi ƒë√≥:

$$
\begin{align*}
& \ P(\text{Pepsi} \rightarrow \text{?} \rightarrow \text{Coca}) \\
=& \ P(\text{Pepsi} \rightarrow \text{Coca} \rightarrow \text{Coca}) + P(\text{Pepsi} \rightarrow \text{Pepsi} \rightarrow \text{Coca}) \\
=& \ 0.2 \times 0.9 + 0.8 \times 0.2 \\
=& \ 0.34
\end{align*}
$$

Ta c≈©ng c√≥ th·ªÉ t√≠nh b·∫±ng c√°ch t√≠nh $\mathbf{P}^2$:

$$
\begin{align*}
& \ P(\text{Pepsi} \rightarrow \text{?} \rightarrow \text{Coca}) \\
=& \ \mathbf{P}^2_{\text{Pepsi}, \text{Coca}} \\
=& \begin{bmatrix}
\_ & \_ \\
0.2 & 0.8
\end{bmatrix} \begin{bmatrix}
0.9 & \_ \\
0.2 & \_
\end{bmatrix} \\
=& \begin{bmatrix}
\_ & \_ \\
0.2 \times 0.9 + 0.8 \times 0.2 & \_
\end{bmatrix} \\
=& \begin{bmatrix}
\_ & \_ \\
0.34 & \_
\end{bmatrix} \\
=& \ 0.34
\end{align*}
$$

#### B√†i to√°n con chu·ªôt

Cho m·ªôt con chu·ªôt s·ªëng trong cƒÉn nh√† g·ªìm $4$ ph√≤ng nh∆∞ sau:

<figure>
<img
    className="w-full md:w-1/2 flex justify-center mx-auto"
    src="/static/images/posts/mouse-room-example.png"
    alt="M√¥ ph·ªèng cƒÉn nh√† c·ªßa con chu·ªôt"
/>
</figure>

M·ªói ng√†y, con chu·ªôt s·∫Ω l·ª±a ch·ªçn ng·∫´u nhi√™n gi·ªØa vi·ªác ·ªü l·∫°i ph√≤ng hi·ªán t·∫°i v·ªõi x√°c su·∫•t $1/4$, ho·∫∑c di chuy·ªÉn ƒë·∫øn m·ªôt ph√≤ng li·ªÅn k·ªÅ v·ªõi x√°c su·∫•t $3/4$.

Gi·∫£ s·ª≠ con chu·ªôt ban ƒë·∫ßu ·ªü ph√≤ng $1$, h√£y t√≠nh x√°c su·∫•t con chu·ªôt s·∫Ω ·ªü ph√≤ng $4$ sau $3$ ng√†y.

Ta l·∫≠p transition matrix v·ªõi $[1, 2, 3, 4]$ l√† c√°c tr·∫°ng th√°i:

$$
\begin{align*}
\mathbf{P} &= \begin{bmatrix}
1/4 & 3/4 & 0 & 0 \\
1/4 & 1/4 & 1/4 & 1/4 \\
0 & 3/8 & 1/4 & 3/8 \\
0 & 3/8 & 3/8 & 1/4
\end{bmatrix}
\end{align*}
$$

T√≠nh $\mathbf{P}^3$:

$$
\begin{align*}
\mathbf{P}^3 &= \begin{bmatrix}
0.128 & 0.377 & 0.248 & 0.248 \\
0.126 & 0.376 & 0.249 & 0.249 \\
0.124 & 0.374 & 0.251 & 0.251 \\
0.124 & 0.374 & 0.251 & 0.251
\end{bmatrix}
\end{align*}
$$

Do ƒë√≥, x√°c su·∫•t con chu·ªôt t·ª´ ph√≤ng $1$ chuy·ªÉn ƒë·∫øn ph√≤ng $4$ sau $3$ ng√†y l√† $0.248$.

## Tri·ªÉn khai language model ƒë∆°n gi·∫£n b·∫±ng Markov Chain

D·ª±a v√†o √Ω t∆∞·ªüng c·ªßa b√†i to√°n con chu·ªôt, ta c√≥ th·ªÉ s·ª≠ d·ª•ng Markov Chain ƒë·ªÉ t·∫°o ra vƒÉn b·∫£n ng·∫´u nhi√™n. B·∫±ng c√°ch nh·∫≠p v√†o m·ªôt t·ª´ b·∫•t k·ª≥, ta s·∫Ω t·∫°o ra m·ªôt c√¢u ng·∫´u nhi√™n v·ªõi ƒë·ªô d√†i tu·ª≥ √Ω.

V√≠ d·ª• khi ta nh·∫≠p t·ª´ "I", s·∫Ω c√≥ m·ªôt x√°c su·∫•t sinh ra t·ª´ "can", v√† v·ªõi t·ª´ "can", s·∫Ω c√≥ m·ªôt x√°c su·∫•t sinh ra t·ª´ "fix" ho·∫∑c "not", v√† v·ªõi t·ª´ "not", s·∫Ω c√≥ m·ªôt x√°c su·∫•t sinh ra t·ª´ "fix",...

<figure>
<img
    className="w-full md:w-1/2 flex justify-center mx-auto"
    src="/static/images/posts/text-room-example.png"
    alt="M√¥ ph·ªèng v√≠ d·ª• v·ªÅ language model ƒë∆°n gi·∫£n b·∫±ng Markov Chain"
/>
</figure>

ƒê·ªÉ c√≥ th·ªÉ tri·ªÉn khai ƒë∆∞·ª£c m√¥ h√¨nh n√†y, ta c·∫ßn m·ªôt ƒëo·∫°n vƒÉn l·ªõn l√†m d·ªØ li·ªáu, v√† [shakespeare.txt](https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt) l√† m·ªôt t·∫≠p d·ªØ li·ªáu ph√π h·ª£p cho v√≠ d·ª• n√†y.

V·ªÅ l√≠ thuy·∫øt, ta s·∫Ω t√°ch c√°c bi-gram (c·∫∑p t·ª´) trong ƒëo·∫°n vƒÉn ƒë√≥ ra l√†m c√°c tr·∫°ng th√°i, v√† t√≠nh x√°c su·∫•t chuy·ªÉn tr·∫°ng th√°i gi·ªØa c√°c tr·∫°ng th√°i ƒë√≥.

Tuy nhi√™n ƒë·ªÉ ƒë∆°n gi·∫£n ho√°, ta s·∫Ω s·ª≠ d·ª•ng m·ªôt `dictionary` v·ªõi c√°c key l√† c√°c t·ª´ duy nh·∫•t trong ƒëo·∫°n vƒÉn, v√† c√°c value l√† m·ªôt `list` ch·ª©a c√°c t·ª´ ti·∫øp theo ƒë√£ xu·∫•t hi·ªán sau t·ª´ ƒë√≥. V√≠ d·ª•:

```json
{
    "I": ["can", "am", "do", "am", ...],
    "can": ["fix", "go", "not", "not", ...]
}
```

To√†n b·ªô code c√≥ th·ªÉ xem chi ti·∫øt t·∫°i: [snowyfield1906/ai-general-research/markov_chain
/markov_chain.py](https://github.com/SnowyField1906/ai-general-research/blob/main/markov_chain/markov_chain.py).

### Ti·ªÅn x·ª≠ l√≠ v√† train model

```python
class Markov():
    def __init__(self, file_path):
        self.file_path = file_path

        self.text = self.remove_punctuations(self.get_text())
        self.text = self.remove_newline(self.text)
        self.model = self.model()

    def get_text(self):
        text = []
        for line in open(self.file_path):
            text.append(line)
        return ' '.join(text)

    def remove_punctuations(self, text):
        return text.translate(str.maketrans('','', string.punctuation))

    def remove_newline(self, text):
        return text.replace('\n', '')

    def model(self):
        words = self.text.split(' ')

        markov_dict = defaultdict(list)

        for current_word, next_word in zip(words[0:-1], words[1:]):
            markov_dict[current_word].append(next_word)

        return dict(markov_dict)
```

### T·∫°o c√¢u ng·∫´u nhi√™n

```python
def predict_words(chain, first_word, number_of_words=5):
    if first_word in list(chain.keys()):
        word1 = str(first_word)

        predictions = word1.capitalize()

        for i in range(number_of_words-1):
            word2 = random.choice(chain[word1])
            word1 = word2
            predictions += ' ' + word2

        predictions += '.'
        return predictions
    else:
        return "Word not in corpus"
```

### Ki·ªÉm th·ª≠

```python
m = Markov('./shakespeare.txt')
chain = m.model
output = predict_words(chain, first_word = 'but')
print(output)
```

```txt
> But that have bought me.
```

## ·ª®ng d·ª•ng

Markov Chain ƒë∆∞·ª£c ·ª©ng d·ª•ng r·ªông r√£i trong nhi·ªÅu lƒ©nh v·ª±c kh√°c nhau, x√©t ri√™ng v·ªÅ Machine Learning, Markov Chain ƒë∆∞·ª£c s·ª≠ d·ª•ng trong r·∫•t nhi·ªÅu b√†i to√°n nh∆∞:

-   **Natural Language Processing** (X·ª≠ l√≠ Ng√¥n ng·ªØ T·ª± nhi√™n)
-   **Recommendation System** (H·ªá th·ªëng G·ª£i √Ω)
-   **Reinforcement Learning** (H·ªçc TƒÉng c∆∞·ªùng)
-   **Computer Vision** (Th·ªã gi√°c M√°y t√≠nh)
-   **Speech Recognition** (Nh·∫≠n d·∫°ng Gi·ªçng n√≥i)
-   ...

Trong ƒë√≥, c√≥ 2 kh√°i ni·ªám quan tr·ªçng l√† **Hidden Markov Model** (M√¥ h√¨nh Markov ·∫®n) v√† **Markov Decision Process** (Quy tr√¨nh Quy·∫øt ƒë·ªãnh Markov). S·∫Ω ƒë∆∞·ª£c gi·ªõi thi·ªáu trong c√°c b√†i vi·∫øt ti·∫øp theo.
